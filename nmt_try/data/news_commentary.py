# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_data.news_commentary.ipynb (unless otherwise specified).

__all__ = ['get_nc_dss']

# Cell
from fastai2.basics import *
from transformers import AutoTokenizer

from fastai2_utils.data.all import *
from fastai_transformers_utils.all import *

# Cell
def _tokenize_data(ori_data_loc, enc_tokenizer, dec_tokenizer):
    df = pd.read_csv(ori_data_loc, header=None, names=['English', 'Chinese'], delimiter='\t', error_bad_lines=False, warn_bad_lines=False)
    df.dropna(inplace=True)
    df.reset_index(drop=True, inplace=True)

    tok_np = np.zeros((len(df), 2)).astype('object')
    for i, eng_str, chn_str in progress_bar(tuple(df.itertuples())):
        tok_en = ' '.join(enc_tokenizer.tokenize(eng_str))
        tok_ch = ' '.join(dec_tokenizer.tokenize(chn_str))
        tok_np[i, 0] = tok_en
        tok_np[i, 1] = tok_ch
    tok_df = pd.DataFrame(tok_np, columns=['English', 'Chinese'])

    return tok_df

# Cell
def _get_eda_tok_df(tok_df):
    eng_len = tok_df.English.apply(lambda x: len(x.split()))
    chn_len = tok_df.Chinese.apply(lambda x: len(x.split()))
    eda_tok_df = pd.DataFrame(data={'eng_len': eng_len, 'chn_len': chn_len}, index=tok_df.index)
    return eda_tok_df

# Cell
def _clean_data(tok_df):
    eda_tok_df = _get_eda_tok_df(tok_df)
    tok_df.drop(index=tok_df[(eda_tok_df.chn_len > 200)].index, inplace=True)
    tok_df.reset_index(drop=True, inplace=True)
    return tok_df

# Cell
def _small_data(tok_df):
    eda_tok_df = _get_eda_tok_df(tok_df)
    tok_df.drop(index=tok_df[(eda_tok_df.chn_len > 62)].index, inplace=True)
    tok_df.reset_index(drop=True, inplace=True)
    return tok_df

# Cell
def _add_isvalid(tok_df):
    is_valid = np.zeros(len(tok_df))
    is_valid[:int(len(tok_df)*0.2)] = 1
    np.random.RandomState(42).shuffle(is_valid)
    is_valid = is_valid.astype(np.bool)
    tok_df['is_valid'] = is_valid
    return tok_df

# Cell
def get_nc_dss(tok_data_loc, enc_tokenizer, dec_tokenizer, enc_seq_len, dec_seq_len, pct=1.0):
    tok_df = pd.read_csv(tok_data_loc)

    splits = ColSplitter()(tok_df)
    splits = pct_splits(splits, pct=pct)

    encoder_input_tfm = [attrgetter('English'), lambda x: x.split(' '), TransformersNumericalize(enc_tokenizer), Pad2Max(enc_seq_len, enc_tokenizer.pad_token_id)]
    decoder_input_tfm = [attrgetter('Chinese'), lambda x: x.split(' '), TransformersNumericalize(dec_tokenizer), Pad2Max(dec_seq_len+1, dec_tokenizer.pad_token_id), lambda x: x[:-1]]
    decoder_output_tfm = [attrgetter('Chinese'), lambda x: x.split(' '), TransformersNumericalize(dec_tokenizer), Pad2Max(dec_seq_len+1, dec_tokenizer.pad_token_id), lambda x: x[1:]]
    ds_tfms = [
        encoder_input_tfm,
        decoder_input_tfm,
        decoder_output_tfm,
    ]

    dss = Datasets(tok_df, tfms=ds_tfms, splits=splits, n_inp=2)
    return dss