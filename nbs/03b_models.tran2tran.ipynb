{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# export\n",
    "import dataclasses\n",
    "\n",
    "from fastai2.basics import *\n",
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "\n",
    "from fastai_transformers_utils.generated_lm import GeneratedLM, GenerateArgs\n",
    "from fastai_transformers_utils.tokenizers import GPT2DecoderTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.tran2tran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tran2Tran\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 3\n",
    "enc_seq_len = 50\n",
    "dec_seq_len = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_max_pos_id = 100\n",
    "enc_vocab_size = 21128\n",
    "enc_pad_id = 0\n",
    "\n",
    "dec_max_pos_id = 100\n",
    "dec_vocab_size = 50259\n",
    "dec_pad_id = 50258\n",
    "\n",
    "embeded_size = 512\n",
    "num_head = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = 2048\n",
    "drop_p = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WPEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# https://github.com/kaushalshetty/Positional-Encoding/blob/master/encoder.py\n",
    "def _position_encoding_init(n_position, emb_dim):\n",
    "    ''' Init the sinusoid position encoding table '''\n",
    "\n",
    "    # keep dim 0 for padding token position encoding zero vector\n",
    "    position_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / emb_dim) for j in range(emb_dim)]\n",
    "        if pos != 0 else np.zeros(emb_dim) for pos in range(n_position)])\n",
    "    \n",
    "\n",
    "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # apply sin on 0th,2nd,4th...emb_dim\n",
    "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # apply cos on 1st,3rd,5th...emb_dim\n",
    "    return torch.from_numpy(position_enc).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class WPEmbed(nn.Module):\n",
    "    '''\n",
    "        returns position_embedding + word_embedding\n",
    "    '''\n",
    "    def __init__(self, vocab_size, embeded_size, max_pos_id, pad_id):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embeded_size, padding_idx=pad_id)\n",
    "        self.position_embedding = nn.Embedding(max_pos_id, embeded_size)\n",
    "        self.position_embedding.weight.data = _position_encoding_init(max_pos_id, embeded_size)\n",
    "        \n",
    "        self.embeded_size = embeded_size\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        '''\n",
    "            input_ids: (b, seq_len)\n",
    "            returns: (b, seq_len, embed_size)\n",
    "        '''\n",
    "        device = input_ids.device\n",
    "        bs, seq_len = input_ids.shape\n",
    "        \n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=device).expand(input_ids.shape) # (b, seq_len)\n",
    "        position_embedding = self.position_embedding(position_ids) # (b, seq_len, embeded_size)\n",
    "        \n",
    "        word_embedding = self.word_embedding(input_ids) # (b, seq_len, embeded_size)\n",
    "        \n",
    "        # This scale factor is very important for your model to converge faster!!!!!!!!!!\n",
    "        scale = torch.sqrt(torch.tensor(self.embeded_size, device=device).float())\n",
    "        \n",
    "        return word_embedding*scale + position_embedding  # (b, seq_len, embeded_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_emb = WPEmbed(enc_vocab_size, embeded_size, enc_max_pos_id, enc_pad_id)\n",
    "input_ids = torch.randint(0, enc_vocab_size, (bs, enc_seq_len)) # (bs, enc_seq_len)\n",
    "embed = enc_emb(input_ids) # (bs, enc_seq_len, embeded_size)\n",
    "test_eq(embed.shape, (bs, enc_seq_len, embeded_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gen_key_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def gen_key_padding_mask(input_ids, pad_id):\n",
    "    ''' Returns ByteTensor where True values are positions that contain pad_id.\n",
    "        input_ids: (bs, seq_len) returns: (bs, seq_len)\n",
    "    '''\n",
    "    device = input_ids.device\n",
    "    mask = torch.where(input_ids == pad_id, torch.tensor(1, device=device), torch.tensor(0, device=device)).to(device)\n",
    "    return mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[12, 11, 0, 0], \n",
    "                          [9, 1, 5, 0]])\n",
    "key_padding_mask = gen_key_padding_mask(input_ids, 0)\n",
    "test_eq(key_padding_mask, torch.tensor([[0, 0, 1, 1],\n",
    "                                        [0, 0, 0, 1]]).bool())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gen_lm_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def gen_lm_mask(tgt_seq_len, device):\n",
    "    \"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "        Unmasked positions are filled with float(0.0).\n",
    "        ex: tgt_seq_len = 4\n",
    "        [[0., -inf, -inf, -inf],\n",
    "        [0., 0., -inf, -inf],\n",
    "        [0., 0., 0., -inf],\n",
    "        [0., 0., 0., 0.]])\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones(tgt_seq_len, tgt_seq_len)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_mask = gen_lm_mask(4, 'cpu')\n",
    "test_eq(lm_mask, torch.tensor([ [0., float('-inf'), float('-inf'), float('-inf')],\n",
    "                                [0., 0., float('-inf'), float('-inf')],\n",
    "                                [0., 0., 0., float('-inf')],\n",
    "                                [0., 0., 0., 0.]]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchFirstTransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BatchFirstTransformerEncoder(nn.TransformerEncoder):\n",
    "    '''\n",
    "        nn.TransformerEncoder want src be (seq_len, bs, embeded_size) and returns (seq_len, bs, embeded_size),\n",
    "        just change it to accept batch first input and returns\n",
    "    '''\n",
    "    def forward(self, src, *inputs, **kwargs):\n",
    "        ''' src: (bs, enc_seq_len, embeded_size), returns: (bs, enc_seq_len, embeded_size) '''\n",
    "        src = src.permute(1, 0, 2) # (enc_seq_len, bs, embeded_size)\n",
    "        output = super().forward(src, *inputs, **kwargs) # (enc_seq_len, bs, embeded_size)\n",
    "        return output.permute(1, 0, 2) # (bs, enc_seq_len, embeded_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(embeded_size, num_head, dim_feedforward, drop_p)\n",
    "encoder_norm = nn.LayerNorm(embeded_size)\n",
    "encoder = BatchFirstTransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "src = torch.randn((bs, enc_seq_len, embeded_size)) # (bs, enc_seq_len, embeded_size)\n",
    "output = encoder(src) # (bs, enc_seq_len, embeded_size)\n",
    "test_eq(output.shape, (bs, enc_seq_len, embeded_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TranEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TranEncoder(nn.Module):\n",
    "    '''\n",
    "        The encoder of transformer architecture.\n",
    "        WPEmbed -> TransformerEncoder.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size, embeded_size, max_pos_id, pad_id, # for WPEmbed\n",
    "        num_head=8, num_encoder_layers=6, dim_feedforward=2048, drop_p=0.1, # for TransformerEncoder\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = WPEmbed(vocab_size, embeded_size, max_pos_id, pad_id)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(embeded_size, num_head, dim_feedforward, drop_p)\n",
    "        encoder_norm = nn.LayerNorm(embeded_size)\n",
    "        self.encoder = BatchFirstTransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "        \n",
    "        self.pad_id = pad_id\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def forward(self, src_input_ids, src_key_padding_mask):\n",
    "        '''\n",
    "            src_input_ids: (bs, enc_seq_len)\n",
    "            src_key_padding_mask: Booling mask that True is padding position with shape (bs, enc_seq_len)\n",
    "            returns: \n",
    "                memory: (bs, enc_seq_len, embeded_size)\n",
    "        '''\n",
    "        embed = self.embedding(src_input_ids) # (bs, enc_seq_len)\n",
    "        memory = self.encoder(embed, src_key_padding_mask=src_key_padding_mask) # (bs, enc_seq_len, embeded_size)\n",
    "        return memory # (bs, enc_seq_len, embeded_size)\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for p in self.encoder.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TranEncoder(\n",
    "    enc_vocab_size, embeded_size, enc_max_pos_id, enc_pad_id, \n",
    "    num_head, num_encoder_layers, dim_feedforward, drop_p\n",
    ")\n",
    "src_input_ids = torch.randint(0, enc_vocab_size, (bs, enc_seq_len)) # (bs, enc_seq_len)\n",
    "src_key_padding_mask = torch.zeros((bs, enc_seq_len)).bool() # (bs, enc_seq_len)\n",
    "memory = encoder(src_input_ids, src_key_padding_mask) # (bs, enc_seq_len, embeded_size)\n",
    "test_eq(memory.shape, (bs, enc_seq_len, embeded_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchFirstTransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BatchFirstTransformerDecoder(nn.TransformerDecoder):\n",
    "    '''\n",
    "        nn.TransformerDecoder want tgt be (seq_len, bs, embeded_size) and returns (seq_len, bs, embeded_size),\n",
    "        just change it to accept batch first input and returns\n",
    "    '''\n",
    "    def forward(self, tgt, memory, *inputs, **kwargs):\n",
    "        ''' \n",
    "            tgt: (bs, dec_seq_len, embeded_size)\n",
    "            memory: (bs, enc_seq_len, embeded_size)\n",
    "            returns: (bs, dec_seq_len, embeded_size) \n",
    "        '''\n",
    "        tgt = tgt.permute(1, 0, 2) # (dec_seq_len, bs, embeded_size)\n",
    "        memory = memory.permute(1, 0, 2) # (enc_seq_len, bs, embeded_size)\n",
    "        output = super().forward(tgt, memory, *inputs, **kwargs) # (dec_seq_len, bs, embeded_size)\n",
    "        return output.permute(1, 0, 2) # (bs, dec_seq_len, embeded_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(embeded_size, num_head, dim_feedforward, drop_p)\n",
    "decoder_norm = nn.LayerNorm(embeded_size)\n",
    "decoder = BatchFirstTransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "tgt = torch.randn((bs, dec_seq_len, embeded_size)) # (bs, dec_seq_len)\n",
    "memory = torch.randn((bs, enc_seq_len, embeded_size)) # (bs, enc_seq_len, embeded_size)\n",
    "output = decoder(tgt, memory) # (bs, dec_seq_len, embeded_size)\n",
    "test_eq(output.shape, (bs, dec_seq_len, embeded_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TranDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TranDecoder(nn.Module):\n",
    "    '''\n",
    "        The decoder of transformer architecture.\n",
    "        WPEmbed -> TransformerDecoder -> Linear Classifier\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size, embeded_size, max_pos_id, pad_id, # for WPEmbed and Classifier\n",
    "        num_head=8, num_decoder_layers=6, dim_feedforward=2048, drop_p=0.1, # for TransformerDecoder\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = WPEmbed(vocab_size, embeded_size, max_pos_id, pad_id)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(embeded_size, num_head, dim_feedforward, drop_p)\n",
    "        decoder_norm = nn.LayerNorm(embeded_size)\n",
    "        self.decoder = BatchFirstTransformerDecoder(decoder_layer, num_decoder_layers)\n",
    "        \n",
    "        self.classifier = nn.Linear(embeded_size, vocab_size) # Language Model\n",
    "        \n",
    "        self.pad_id = pad_id\n",
    "    \n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, tgt_input_ids, memory, memory_key_padding_mask):\n",
    "        '''\n",
    "            tgt_input_ids: (bs, dec_seq_len)\n",
    "            memory: (bs, enc_seq_len, embeded_size)\n",
    "            memory_key_padding_mask: (bs, enc_seq_len)\n",
    "            memory_mask: (dec_seq_len, enc_seq_len)\n",
    "            returns: (bs, dec_seq_len, dec_vocab_size), None\n",
    "        '''\n",
    "        seq_len = tgt_input_ids.shape[1]\n",
    "        embed = self.embedding(tgt_input_ids) # (bs, dec_seq_len)\n",
    "        tgt_key_padding_mask = gen_key_padding_mask(tgt_input_ids, self.pad_id) # (bs, dec_seq_len)\n",
    "        tgt_mask = gen_lm_mask(seq_len, tgt_input_ids.device) #(dec_seq_len, dec_seq_len)\n",
    "        \n",
    "        output = self.decoder(embed, memory, tgt_mask=tgt_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask) # (bs, dec_seq_len, embeded_size)\n",
    "        \n",
    "        output = self.classifier(output) # (bs, dec_seq_len, dec_vocab_size)\n",
    "        return output, None # (bs, dec_seq_len, dec_vocab_size)\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for p in self.decoder.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        for p in self.classifier.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.kaiming_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TranDecoder(\n",
    "    dec_vocab_size, embeded_size, dec_max_pos_id, dec_pad_id, \n",
    "    num_head, num_decoder_layers, dim_feedforward, drop_p\n",
    ")\n",
    "\n",
    "tgt_input_ids = torch.randint(0, dec_vocab_size, (bs, dec_seq_len)) # (bs, dec_seq_len)\n",
    "memory = torch.randn((bs, enc_seq_len, embeded_size)) # (bs, enc_seq_len, embeded_size)\n",
    "memory_key_padding_mask = torch.zeros((bs, enc_seq_len)).bool() # (bs, enc_seq_len)\n",
    "\n",
    "output = decoder(tgt_input_ids, memory, memory_key_padding_mask=memory_key_padding_mask) # (bs, dec_seq_len, dec_vocab_size)\n",
    "test_eq(output[0].shape, (bs, dec_seq_len, dec_vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tran2Tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Tran2Tran(nn.Module):\n",
    "    '''\n",
    "        Seq2Seq architecture. Encoder and decoder are transformer.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        encoder: TranEncoder, decoder: TranDecoder, \n",
    "        enc_pad_id, # for memory_key_padding_mask\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.enc_pad_id = enc_pad_id\n",
    "        \n",
    "    def forward(self, src_input_ids, tgt_input_ids):\n",
    "        '''\n",
    "           src_input_ids: (bs, enc_seq_len)\n",
    "           tgt_input_ids: (bs, dec_seq_len)\n",
    "           returns: (bs, dec_seq_len, dec_vocab_size)\n",
    "        '''\n",
    "        \n",
    "        src_key_padding_mask = gen_key_padding_mask(src_input_ids, self.enc_pad_id) # (bs, enc_seq_len)\n",
    "        memory = self.encoder(src_input_ids, src_key_padding_mask=src_key_padding_mask) # (bs, enc_seq_len, embeded_size)\n",
    "        output, _ = self.decoder(tgt_input_ids, memory, memory_key_padding_mask=src_key_padding_mask) # (bs, dec_seq_len, embeded_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = TranEncoder(enc_vocab_size, embeded_size, enc_max_pos_id, enc_pad_id)\n",
    "# decoder = TranDecoder(dec_vocab_size, embeded_size, dec_max_pos_id, dec_pad_id)\n",
    "tran2tran = Tran2Tran(encoder, decoder, enc_pad_id)\n",
    "\n",
    "src_input_ids = torch.randint(0, enc_vocab_size, (bs, enc_seq_len)) # (bs, enc_seq_len)\n",
    "tgt_input_ids = torch.randint(0, dec_vocab_size, (bs, dec_seq_len)) # (bs, dec_seq_len)\n",
    "output = tran2tran(src_input_ids, tgt_input_ids) # (bs, dec_seq_len, dec_vocab_size)\n",
    "test_eq(output.shape, (bs, dec_seq_len, dec_vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeneratedTran2Tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class GeneratedTran2Tran():\n",
    "    def __init__(\n",
    "        self, \n",
    "        seq2seq: Tran2Tran, \n",
    "        enc_tokenizer: PreTrainedTokenizer,\n",
    "        dec_tokenizer: PreTrainedTokenizer,\n",
    "    ):\n",
    "        self.seq2seq = seq2seq\n",
    "        self.enc_tokenizer = enc_tokenizer\n",
    "        self.dec_tokenizer = dec_tokenizer\n",
    "        self.generatedLM = GeneratedLM(seq2seq.decoder, len(dec_tokenizer), dec_tokenizer.pad_token_id, [dec_tokenizer.eos_token_id], support_past=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-bert-wwm-ext')\n",
    "dec_tokenizer = GPT2DecoderTokenizer.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_from_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@patch\n",
    "@torch.no_grad()\n",
    "def generate_from_ids(self: GeneratedTran2Tran, src_input_ids, generate_args: GenerateArgs):\n",
    "    ''' src_input_ids: (bs, enc_seq_len), returns: (bs, max_length)'''\n",
    "    self.seq2seq.eval()\n",
    "    \n",
    "    device = src_input_ids.device\n",
    "    bs = src_input_ids.shape[0]\n",
    "    tgt_input_ids = torch.zeros((bs, 1), dtype=torch.long, device=device).fill_(self.dec_tokenizer.bos_token_id) # (bs, 1)\n",
    "\n",
    "    src_key_padding_mask = gen_key_padding_mask(src_input_ids, self.enc_tokenizer.pad_token_id) # (bs, enc_seq_len)\n",
    "    memory = self.seq2seq.encoder(src_input_ids, src_key_padding_mask) # (bs, enc_seq_len, embeded_size)\n",
    "    model_otherargs = self.generatedLM.build_model_otherargs_for_beam([memory, src_key_padding_mask], generate_args.num_beams)\n",
    "\n",
    "    result = self.generatedLM.generate(tgt_input_ids, generate_args, [model_otherargs[0]], dict(memory_key_padding_mask=model_otherargs[1]))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tran2tran = GeneratedTran2Tran(tran2tran, enc_tokenizer, dec_tokenizer)\n",
    "\n",
    "generate_args = GenerateArgs(max_length=10, num_beams=2)\n",
    "src_input_ids = torch.randint(0, enc_vocab_size, (bs, enc_seq_len)) # (bs, enc_seq_len)\n",
    "result = generated_tran2tran.generate_from_ids(src_input_ids, generate_args)\n",
    "test_eq(result.shape, (bs, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 02_data.tatoeba.ipynb.\n",
      "Converted 03a_models.core.ipynb.\n",
      "Converted 03b_models.tran2tran.ipynb.\n",
      "Converted 03c_models.gru2gru.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "355px",
    "width": "380px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
