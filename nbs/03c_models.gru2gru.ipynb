{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "from fastai2.basics import *\n",
    "\n",
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "\n",
    "from fastai_transformers_utils.generated_lm import GeneratedLM, GenerateArgs\n",
    "from fastai_transformers_utils.tokenizers import GPT2DecoderTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.gru2gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 3\n",
    "enc_seq_len = 50\n",
    "dec_seq_len = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Models GRU2GRU\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = 21128\n",
    "enc_pad_id = 0\n",
    "\n",
    "dec_vocab_size = 50259\n",
    "dec_pad_id = 50258\n",
    "\n",
    "embeded_size = 512\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "drop_p = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class GRUEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size, embeded_size, pad_id,\n",
    "        num_layers=1, drop_p=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embeded_size, padding_idx=pad_id)\n",
    "        self.encoder = nn.GRU(embeded_size, embeded_size, num_layers=num_layers, dropout=drop_p, batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, src_inp_ids):\n",
    "        '''\n",
    "            src_inp_ids: (bs, enc_seq_len)\n",
    "            returns: output, h\n",
    "                output: (bs, seq_len, 2*embeded_size)\n",
    "                h: (2*num_layers, bs, embeded_size)\n",
    "        '''\n",
    "        embeded = self.embedding(src_inp_ids) # (bs, enc_seq_len, embeded_size)\n",
    "        output, h = self.encoder(embeded) # (bs, enc_seq_len, 2*embeded_size), (2*num_encoder_layers, bs, embeded_size)\n",
    "        return output, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input_ids = torch.randint(0, enc_vocab_size, (bs, enc_seq_len)) # (bs, enc_seq_len)\n",
    "encoder = GRUEncoder(enc_vocab_size, embeded_size, enc_pad_id, num_encoder_layers, drop_p)\n",
    "output, h = encoder(src_input_ids)\n",
    "test_eq(output.shape, (bs, enc_seq_len, 2*embeded_size))\n",
    "test_eq(h.shape, (2*num_encoder_layers, bs, embeded_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class GRUDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size, embeded_size, pad_id,\n",
    "        num_layers=1, drop_p=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embeded_size, padding_idx=pad_id)\n",
    "        self.decoder = nn.GRU(embeded_size, embeded_size, num_layers=num_layers, dropout=drop_p, batch_first=True)\n",
    "        self.classifier = nn.Linear(embeded_size, vocab_size)\n",
    "        \n",
    "    def forward(self, tgt_inp_ids, h):\n",
    "        '''\n",
    "            tgt_inp_ids: (bs, dec_seq_len)\n",
    "            h: (num_decoder_layers, bs, embeded_size)\n",
    "            returns: output, h\n",
    "                output: (bs, dec_seq_len, dec_vocab_size)\n",
    "                h: (num_decoder_layers, bs, embeded_size)\n",
    "        '''\n",
    "        embeded = self.embedding(tgt_inp_ids) # (bs, dec_seq_len, embeded_size)\n",
    "        output, h = self.decoder(embeded, h) # (bs, dec_seq_len, embeded_size), (num_decoder_layers, bs, embeded_size)\n",
    "        output = self.classifier(output) # (bs, dec_seq_len, dec_vocab_size)\n",
    "        return output, h # (bs, dec_seq_len, dec_vocab_size), (num_decoder_layers, bs, embeded_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = GRUDecoder(dec_vocab_size, embeded_size, dec_pad_id, num_decoder_layers, drop_p)\n",
    "\n",
    "tgt_input_ids = torch.randint(0, dec_vocab_size, (bs, dec_seq_len)) # (bs, dec_seq_len)\n",
    "h = torch.randn((num_decoder_layers, bs, embeded_size)) # (num_decoder_layers, bs, embeded_size)\n",
    "\n",
    "output, h = decoder(tgt_input_ids, h)\n",
    "test_eq(output.shape, (bs, dec_seq_len, dec_vocab_size))\n",
    "test_eq(h.shape, (num_decoder_layers, bs, embeded_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU2GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class GRU2GRU(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        encoder: GRUEncoder, decoder: GRUDecoder, \n",
    "        num_encoder_layers, num_decoder_layers,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.proj = nn.Linear(2*num_encoder_layers, num_decoder_layers)\n",
    "        \n",
    "    def forward(self, src_input_ids, tgt_input_ids):\n",
    "        '''\n",
    "            src_input_ids: (bs, enc_seq_len)\n",
    "            tgt_input_ids: (bs, dec_seq_len)\n",
    "            returns: (bs, dec_seq_len, dec_vocab_size)\n",
    "        '''\n",
    "        _, enc_h = self.encoder(src_input_ids) # (2*num_encoder_layers, bs, embeded_size)\n",
    "        enc_h = enc_h.permute(1, 2, 0) # (bs, embeded_size, 2*num_encoder_layers)\n",
    "        enc_h_proj = self.proj(enc_h) # (bs, embeded_size, num_decoder_layers)\n",
    "        enc_h_proj = enc_h_proj.permute(2, 0, 1) # (num_decoder_layers, bs, embeded_size)\n",
    "        enc_h_proj = enc_h_proj.contiguous()\n",
    "        \n",
    "        output, dec_h = self.decoder(tgt_input_ids, enc_h_proj) # (bs, dec_seq_len, dec_vocab_size), (num_decoder_layers, bs, embeded_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru2gru = GRU2GRU(encoder, decoder, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "src_input_ids = torch.randint(0, enc_vocab_size, (bs, enc_seq_len)) # (bs, enc_seq_len)\n",
    "tgt_input_ids = torch.randint(0, dec_vocab_size, (bs, dec_seq_len)) # (bs, dec_seq_len)\n",
    "output = gru2gru(src_input_ids, tgt_input_ids) # (bs, dec_seq_len, dec_vocab_size)\n",
    "test_eq(output.shape, (bs, dec_seq_len, dec_vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeneratedGRU2GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class GeneratedGRU2GRU():\n",
    "    '''\n",
    "        device is for created tensors\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq2seq: GRU2GRU, \n",
    "        enc_tokenizer: PreTrainedTokenizer,\n",
    "        dec_tokenizer: PreTrainedTokenizer,\n",
    "    ):\n",
    "        self.seq2seq = seq2seq\n",
    "        self.enc_tokenizer = enc_tokenizer\n",
    "        self.dec_tokenizer = dec_tokenizer\n",
    "        self.generatedLM = GeneratedLM(seq2seq.decoder, len(dec_tokenizer), dec_tokenizer.pad_token_id, [dec_tokenizer.eos_token_id], support_past=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-bert-wwm-ext')\n",
    "dec_tokenizer = GPT2DecoderTokenizer.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_from_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@patch\n",
    "@torch.no_grad()\n",
    "def generate_from_ids(self: GeneratedGRU2GRU, src_input_ids, generate_args: GenerateArgs):\n",
    "    ''' src_input_ids: (bs, enc_seq_len) '''\n",
    "    self.seq2seq.eval()\n",
    "    \n",
    "    device = src_input_ids.device\n",
    "    bs = src_input_ids.shape[0]\n",
    "    tgt_input_ids = torch.zeros((bs, 1), dtype=torch.long).fill_(self.dec_tokenizer.bos_token_id).to(device) # (bs, 1)\n",
    "\n",
    "    _, enc_h = self.seq2seq.encoder(src_input_ids) # (2*num_encoder_layers, bs, embeded_size)\n",
    "    enc_h = enc_h.permute(1, 2, 0) # (bs, embeded_size, 2*num_encoder_layers)\n",
    "    enc_h_proj =  self.seq2seq.proj(enc_h) # (bs, embeded_size, num_decoder_layers)\n",
    "    model_otherargs = self.generatedLM.build_model_otherargs_for_beam([enc_h_proj], generate_args.num_beams) # (bs*num_beams, embeded_size, num_decoder_layers)\n",
    "    enc_h_proj = model_otherargs[0].permute(2, 0, 1) # (num_decoder_layers, bs*num_beams, embeded_size)\n",
    "    enc_h_proj = enc_h_proj.contiguous()\n",
    "\n",
    "    result = self.generatedLM.generate(tgt_input_ids, generate_args, [enc_h_proj])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_gru2gru = GeneratedGRU2GRU(gru2gru, enc_tokenizer, dec_tokenizer)\n",
    "\n",
    "generate_args = GenerateArgs(max_length=20, num_beams=2)\n",
    "src_input_ids = torch.randint(0, enc_vocab_size, (bs, enc_seq_len)) # (bs, enc_seq_len)\n",
    "result = generated_gru2gru.generate_from_ids(src_input_ids, generate_args)\n",
    "test_eq(result.shape, (bs, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 02_data.tatoeba.ipynb.\n",
      "Converted 03a_models.core.ipynb.\n",
      "Converted 03b_models.tran2tran.ipynb.\n",
      "Converted 03c_models.gru2gru.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
