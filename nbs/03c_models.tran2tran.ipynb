{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# export\n",
    "import dataclasses\n",
    "\n",
    "from fastai2.basics import *\n",
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "\n",
    "from fastai2_utils.pytorch.transformer import *\n",
    "from fastai_transformers_utils.generated_lm import GeneratedLM, GenerateArgs\n",
    "from fastai_transformers_utils.tokenizers import GPT2DecoderTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.tran2tran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Tran2Tran\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 3\n",
    "enc_seq_len = 50\n",
    "dec_seq_len = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_max_pos_id = 100\n",
    "enc_vocab_size = 21128\n",
    "enc_pad_id = 0\n",
    "\n",
    "dec_max_pos_id = 100\n",
    "dec_vocab_size = 50259\n",
    "dec_pad_id = 50258\n",
    "\n",
    "embeded_size = 512\n",
    "num_head = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = 2048\n",
    "drop_p = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WPEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# https://github.com/kaushalshetty/Positional-Encoding/blob/master/encoder.py\n",
    "def _position_encoding_init(n_position, emb_dim):\n",
    "    ''' Init the sinusoid position encoding table '''\n",
    "\n",
    "    # keep dim 0 for padding token position encoding zero vector\n",
    "    position_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / emb_dim) for j in range(emb_dim)]\n",
    "        if pos != 0 else np.zeros(emb_dim) for pos in range(n_position)])\n",
    "    \n",
    "\n",
    "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # apply sin on 0th,2nd,4th...emb_dim\n",
    "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # apply cos on 1st,3rd,5th...emb_dim\n",
    "    return torch.from_numpy(position_enc).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class WPEmbed(nn.Module):\n",
    "    '''\n",
    "        returns position_embedding + word_embedding\n",
    "    '''\n",
    "    def __init__(self, vocab_size, embeded_size, max_pos_id, pad_id):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embeded_size, padding_idx=pad_id)\n",
    "        self.position_embedding = nn.Embedding(max_pos_id, embeded_size)\n",
    "        self.position_embedding.weight.data = _position_encoding_init(max_pos_id, embeded_size)\n",
    "        \n",
    "        self.embeded_size = embeded_size\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        '''\n",
    "            input_ids: (b, seq_len)\n",
    "            returns: (b, seq_len, embed_size)\n",
    "        '''\n",
    "        device = input_ids.device\n",
    "        bs, seq_len = input_ids.shape\n",
    "        \n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=device).expand(input_ids.shape) # (b, seq_len)\n",
    "        position_embedding = self.position_embedding(position_ids) # (b, seq_len, embeded_size)\n",
    "        \n",
    "        word_embedding = self.word_embedding(input_ids) # (b, seq_len, embeded_size)\n",
    "        \n",
    "        # This scale factor is very important for your model to converge faster!!!!!!!!!!\n",
    "        scale = torch.sqrt(torch.tensor(self.embeded_size, device=device).float())\n",
    "        \n",
    "        return word_embedding*scale + position_embedding  # (b, seq_len, embeded_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_emb = WPEmbed(enc_vocab_size, embeded_size, enc_max_pos_id, enc_pad_id)\n",
    "input_ids = torch.randint(0, enc_vocab_size, (bs, enc_seq_len)) # (bs, enc_seq_len)\n",
    "embed = enc_emb(input_ids) # (bs, enc_seq_len, embeded_size)\n",
    "test_eq(embed.shape, (bs, enc_seq_len, embeded_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TranEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TranEncoder(nn.Module):\n",
    "    '''\n",
    "        The encoder of transformer architecture.\n",
    "        WPEmbed -> TransformerEncoder.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size, embeded_size, max_pos_id, pad_id, # for WPEmbed\n",
    "        num_head=8, num_encoder_layers=6, dim_feedforward=2048, drop_p=0.1, # for TransformerEncoder\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = WPEmbed(vocab_size, embeded_size, max_pos_id, pad_id)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(embeded_size, num_head, dim_feedforward, drop_p)\n",
    "        encoder_norm = nn.LayerNorm(embeded_size)\n",
    "        self.encoder = BatchFirstTransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "        \n",
    "        self.pad_id = pad_id\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def forward(self, src_input_ids, src_key_padding_mask):\n",
    "        '''\n",
    "            src_input_ids: (bs, enc_seq_len)\n",
    "            src_key_padding_mask: Booling mask that True is padding position with shape (bs, enc_seq_len)\n",
    "            returns: \n",
    "                memory: (bs, enc_seq_len, embeded_size)\n",
    "        '''\n",
    "        embed = self.embedding(src_input_ids) # (bs, enc_seq_len)\n",
    "        memory = self.encoder(embed, src_key_padding_mask=src_key_padding_mask) # (bs, enc_seq_len, embeded_size)\n",
    "        return memory # (bs, enc_seq_len, embeded_size)\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for p in self.encoder.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TranEncoder(\n",
    "    enc_vocab_size, embeded_size, enc_max_pos_id, enc_pad_id, \n",
    "    num_head, num_encoder_layers, dim_feedforward, drop_p\n",
    ")\n",
    "src_input_ids = torch.randint(0, enc_vocab_size, (bs, enc_seq_len)) # (bs, enc_seq_len)\n",
    "src_key_padding_mask = torch.zeros((bs, enc_seq_len)).bool() # (bs, enc_seq_len)\n",
    "memory = encoder(src_input_ids, src_key_padding_mask) # (bs, enc_seq_len, embeded_size)\n",
    "test_eq(memory.shape, (bs, enc_seq_len, embeded_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TranDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TranDecoder(nn.Module):\n",
    "    '''\n",
    "        The decoder of transformer architecture.\n",
    "        WPEmbed -> TransformerDecoder -> Linear Classifier\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size, embeded_size, max_pos_id, pad_id, # for WPEmbed and Classifier\n",
    "        num_head=8, num_decoder_layers=6, dim_feedforward=2048, drop_p=0.1, # for TransformerDecoder\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = WPEmbed(vocab_size, embeded_size, max_pos_id, pad_id)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(embeded_size, num_head, dim_feedforward, drop_p)\n",
    "        decoder_norm = nn.LayerNorm(embeded_size)\n",
    "        self.decoder = BatchFirstTransformerDecoder(decoder_layer, num_decoder_layers)\n",
    "        \n",
    "        self.classifier = nn.Linear(embeded_size, vocab_size) # Language Model\n",
    "        \n",
    "        self.pad_id = pad_id\n",
    "    \n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, tgt_input_ids, memory, memory_key_padding_mask):\n",
    "        '''\n",
    "            tgt_input_ids: (bs, dec_seq_len)\n",
    "            memory: (bs, enc_seq_len, embeded_size)\n",
    "            memory_key_padding_mask: (bs, enc_seq_len)\n",
    "            memory_mask: (dec_seq_len, enc_seq_len)\n",
    "            returns: (bs, dec_seq_len, dec_vocab_size), None\n",
    "        '''\n",
    "        seq_len = tgt_input_ids.shape[1]\n",
    "        embed = self.embedding(tgt_input_ids) # (bs, dec_seq_len)\n",
    "        tgt_key_padding_mask = gen_key_padding_mask(tgt_input_ids, self.pad_id) # (bs, dec_seq_len)\n",
    "        tgt_mask = gen_lm_mask(seq_len, tgt_input_ids.device) #(dec_seq_len, dec_seq_len)\n",
    "        \n",
    "        output = self.decoder(embed, memory, tgt_mask=tgt_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask) # (bs, dec_seq_len, embeded_size)\n",
    "        \n",
    "        output = self.classifier(output) # (bs, dec_seq_len, dec_vocab_size)\n",
    "        return output, None # (bs, dec_seq_len, dec_vocab_size)\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for p in self.decoder.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        for p in self.classifier.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.kaiming_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TranDecoder(\n",
    "    dec_vocab_size, embeded_size, dec_max_pos_id, dec_pad_id, \n",
    "    num_head, num_decoder_layers, dim_feedforward, drop_p\n",
    ")\n",
    "\n",
    "tgt_input_ids = torch.randint(0, dec_vocab_size, (bs, dec_seq_len)) # (bs, dec_seq_len)\n",
    "memory = torch.randn((bs, enc_seq_len, embeded_size)) # (bs, enc_seq_len, embeded_size)\n",
    "memory_key_padding_mask = torch.zeros((bs, enc_seq_len)).bool() # (bs, enc_seq_len)\n",
    "\n",
    "output = decoder(tgt_input_ids, memory, memory_key_padding_mask=memory_key_padding_mask) # (bs, dec_seq_len, dec_vocab_size)\n",
    "test_eq(output[0].shape, (bs, dec_seq_len, dec_vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tran2Tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Tran2Tran(nn.Module):\n",
    "    '''\n",
    "        Seq2Seq architecture. Encoder and decoder are transformer.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        encoder: TranEncoder, decoder: TranDecoder, \n",
    "        enc_pad_id, # for src_key_padding_mask and memory_key_padding_mask\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.enc_pad_id = enc_pad_id\n",
    "        \n",
    "    def forward(self, src_input_ids, tgt_input_ids):\n",
    "        '''\n",
    "           src_input_ids: (bs, enc_seq_len)\n",
    "           tgt_input_ids: (bs, dec_seq_len)\n",
    "           returns: (bs, dec_seq_len, dec_vocab_size)\n",
    "        '''\n",
    "        \n",
    "        src_key_padding_mask = gen_key_padding_mask(src_input_ids, self.enc_pad_id) # (bs, enc_seq_len)\n",
    "        memory = self.encoder(src_input_ids, src_key_padding_mask=src_key_padding_mask) # (bs, enc_seq_len, embeded_size)\n",
    "        output, _ = self.decoder(tgt_input_ids, memory, memory_key_padding_mask=src_key_padding_mask) # (bs, dec_seq_len, embeded_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = TranEncoder(enc_vocab_size, embeded_size, enc_max_pos_id, enc_pad_id)\n",
    "# decoder = TranDecoder(dec_vocab_size, embeded_size, dec_max_pos_id, dec_pad_id)\n",
    "tran2tran = Tran2Tran(encoder, decoder, enc_pad_id)\n",
    "\n",
    "src_input_ids = torch.randint(0, enc_vocab_size, (bs, enc_seq_len)) # (bs, enc_seq_len)\n",
    "tgt_input_ids = torch.randint(0, dec_vocab_size, (bs, dec_seq_len)) # (bs, dec_seq_len)\n",
    "output = tran2tran(src_input_ids, tgt_input_ids) # (bs, dec_seq_len, dec_vocab_size)\n",
    "test_eq(output.shape, (bs, dec_seq_len, dec_vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeneratedTran2Tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class GeneratedTran2Tran():\n",
    "    def __init__(\n",
    "        self, \n",
    "        seq2seq: Tran2Tran, \n",
    "        enc_tokenizer: PreTrainedTokenizer,\n",
    "        dec_tokenizer: PreTrainedTokenizer,\n",
    "    ):\n",
    "        self.seq2seq = seq2seq\n",
    "        self.enc_tokenizer = enc_tokenizer\n",
    "        self.dec_tokenizer = dec_tokenizer\n",
    "        self.generatedLM = GeneratedLM(seq2seq.decoder, len(dec_tokenizer), dec_tokenizer.pad_token_id, [dec_tokenizer.eos_token_id], support_past=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-bert-wwm-ext')\n",
    "dec_tokenizer = GPT2DecoderTokenizer.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_from_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@patch\n",
    "@torch.no_grad()\n",
    "def generate_from_ids(self: GeneratedTran2Tran, src_input_ids, generate_args: GenerateArgs):\n",
    "    ''' src_input_ids: (bs, enc_seq_len), returns: (bs, max_length)'''\n",
    "    self.seq2seq.eval()\n",
    "    \n",
    "    device = src_input_ids.device\n",
    "    bs = src_input_ids.shape[0]\n",
    "    tgt_input_ids = torch.zeros((bs, 1), dtype=torch.long, device=device).fill_(self.dec_tokenizer.bos_token_id) # (bs, 1)\n",
    "\n",
    "    src_key_padding_mask = gen_key_padding_mask(src_input_ids, self.enc_tokenizer.pad_token_id) # (bs, enc_seq_len)\n",
    "    memory = self.seq2seq.encoder(src_input_ids, src_key_padding_mask) # (bs, enc_seq_len, embeded_size)\n",
    "    model_otherargs = self.generatedLM.build_model_otherargs_for_beam([memory, src_key_padding_mask], generate_args.num_beams)\n",
    "\n",
    "    result = self.generatedLM.generate(tgt_input_ids, generate_args, [model_otherargs[0]], dict(memory_key_padding_mask=model_otherargs[1]))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tran2tran = GeneratedTran2Tran(tran2tran, enc_tokenizer, dec_tokenizer)\n",
    "\n",
    "generate_args = GenerateArgs(max_length=10, num_beams=2)\n",
    "src_input_ids = torch.randint(0, enc_vocab_size, (bs, enc_seq_len)) # (bs, enc_seq_len)\n",
    "result = generated_tran2tran.generate_from_ids(src_input_ids, generate_args)\n",
    "test_eq(result.shape, (bs, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 02_data.tatoeba.ipynb.\n",
      "Converted 03a_models.patch.ipynb.\n",
      "Converted 03c_models.bert2gpt2.ipynb.\n",
      "Converted 03c_models.gru2gru.ipynb.\n",
      "Converted 03c_models.tran2tran.ipynb.\n",
      "Converted 04_metrics.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
