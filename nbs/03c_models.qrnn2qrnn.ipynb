{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "from fastai2.basics import *\n",
    "from fastai2.text.all import *\n",
    "from fastai2.text.models.qrnn import *\n",
    "\n",
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "\n",
    "from fastai2_utils.pytorch.transformer import *\n",
    "from fastai_transformers_utils.generated_lm import GeneratedLM, GenerateArgs\n",
    "from fastai_transformers_utils.tokenizers import GPT2DecoderTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.qrnn2qrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 3\n",
    "enc_seq_len = 50\n",
    "dec_seq_len = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models QRNN2QRNN\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = 21128\n",
    "enc_pad_id = 0\n",
    "\n",
    "dec_vocab_size = 50259\n",
    "dec_pad_id = 50258\n",
    "\n",
    "embeded_size = 512\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "drop_p = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QRNNEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class QRNNEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size, embeded_size, pad_id,\n",
    "        num_layers=1, drop_p=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embeded_size, padding_idx=pad_id)\n",
    "        self.encoder = QRNN(embeded_size, embeded_size, n_layers=num_layers, dropout=drop_p, bidirectional=True, save_prev_x=False)\n",
    "        \n",
    "    def forward(self, src_inp_ids):\n",
    "        '''\n",
    "            src_inp_ids: (bs, enc_seq_len)\n",
    "            returns: output, h\n",
    "                output: (bs, seq_len, 2*embeded_size)\n",
    "                h: (2*num_layers, bs, embeded_size)\n",
    "        '''\n",
    "        embeded = self.embedding(src_inp_ids) # (bs, enc_seq_len, embeded_size)\n",
    "        output, h = self.encoder(embeded) # (bs, enc_seq_len, 2*embeded_size), (2*num_encoder_layers, bs, embeded_size)\n",
    "        return output, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input_ids = torch.randint(0, enc_vocab_size, (bs, enc_seq_len)) # (bs, enc_seq_len)\n",
    "encoder = QRNNEncoder(enc_vocab_size, embeded_size, enc_pad_id, num_encoder_layers, drop_p)\n",
    "output, h = encoder(src_input_ids)\n",
    "test_eq(output.shape, (bs, enc_seq_len, 2*embeded_size))\n",
    "test_eq(h.shape, (2*num_encoder_layers, bs, embeded_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QRNNDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class QRNNDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size, embeded_size, pad_id,\n",
    "        num_layers=1, drop_p=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embeded_size, padding_idx=pad_id)\n",
    "        self.decoder = QRNN(embeded_size, embeded_size, n_layers=num_layers, dropout=drop_p, save_prev_x=True)\n",
    "        self.classifier = nn.Linear(embeded_size, vocab_size)\n",
    "        \n",
    "    def forward(self, tgt_inp_ids, h):\n",
    "        '''\n",
    "            tgt_inp_ids: (bs, dec_seq_len)\n",
    "            h: (num_decoder_layers, bs, embeded_size)\n",
    "            returns: output, h\n",
    "                output: (bs, dec_seq_len, dec_vocab_size)\n",
    "                h: (num_decoder_layers, bs, embeded_size)\n",
    "        '''\n",
    "        embeded = self.embedding(tgt_inp_ids) # (bs, dec_seq_len, embeded_size)\n",
    "        output, h = self.decoder(embeded, h) # (bs, dec_seq_len, embeded_size), (num_decoder_layers, bs, embeded_size)\n",
    "        output = self.classifier(output) # (bs, dec_seq_len, dec_vocab_size)\n",
    "        return output, h # (bs, dec_seq_len, dec_vocab_size), (num_decoder_layers, bs, embeded_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = QRNNDecoder(dec_vocab_size, embeded_size, dec_pad_id, num_decoder_layers, drop_p)\n",
    "\n",
    "tgt_input_ids = torch.randint(0, dec_vocab_size, (bs, dec_seq_len)) # (bs, dec_seq_len)\n",
    "h = torch.randn((num_decoder_layers, bs, embeded_size)) # (num_decoder_layers, bs, embeded_size)\n",
    "\n",
    "output, h = decoder(tgt_input_ids, h)\n",
    "test_eq(output.shape, (bs, dec_seq_len, dec_vocab_size))\n",
    "test_eq(h.shape, (num_decoder_layers, bs, embeded_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossAttnQRNNDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Decoder, uni-directional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossAttnQRNNLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CrossAttnQRNNLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        targ_inp_size, embed_dim, window=1, # for qrnn_layer\n",
    "        num_heads=1, drop_p=0, # for cross_attn\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.qrnn_layer = QRNNLayer(targ_inp_size, hidden_size=embed_dim, save_prev_x=True, zoneout=0, window=window,\n",
    "                                     output_gate=True, batch_first=True, backward=False)\n",
    "        self.cross_attn = BatchFirstMultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=drop_p)\n",
    "    def forward(self, targ, memory, memory_key_padding_mask, hid=None):\n",
    "        '''\n",
    "            targ: (bs, targ_seq_len, targ_inp_size)\n",
    "            memory:(bs, src_seq_len, embed_dim)\n",
    "            memory_key_padding_mask: (bs, src_seq_len)\n",
    "            hid: (bs, embed_dim)\n",
    "            returns:\n",
    "                output: (bs, tgt_seq_len, embed_dim)\n",
    "                attn_weight: (bs, tgt_seq_len, src_seq_len)\n",
    "                new_hid: (bs, targ_seq_len)\n",
    "        '''\n",
    "        qrnn_out, new_hid = self.qrnn_layer(targ, hid) # qrnn_out: (bs, targ_seq_len, embed_dim), new_hid: (bs, embed_dim)\n",
    "        output, attn_weight = self.cross_attn(qrnn_out, memory, memory, key_padding_mask=memory_key_padding_mask) \n",
    "        return output, attn_weight, new_hid \n",
    "    def reset(self):\n",
    "        self.qrnn_layer.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ = torch.randn((16, 18, 20))\n",
    "memory = torch.randn((16, 28, 10))\n",
    "memory_key_padding_mask = torch.zeros((16, 28)).bool()\n",
    "cross_attn_qrnn_layer = CrossAttnQRNNLayer(20, 10)\n",
    "output, attn_weight, new_hid = cross_attn_qrnn_layer(targ, memory, memory_key_padding_mask)\n",
    "test_eq(output.shape, (16, 18, 10))\n",
    "test_eq(attn_weight.shape, (16, 18, 28))\n",
    "test_eq(new_hid.shape, (16, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossAttnQRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CrossAttnQRNN(nn.Module):\n",
    "    def __init__(self, targ_inp_size, embed_dim, n_layers=1, num_heads=1, drop_p=0):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([CrossAttnQRNNLayer(targ_inp_size if l == 0 else embed_dim, embed_dim,\n",
    "                                                       window=(2 if l ==0 else 1), num_heads=num_heads, drop_p=drop_p)\n",
    "                                     for l in range(n_layers)])\n",
    "        self.n_layers = n_layers\n",
    "    def forward(self, targ, memory, memory_key_padding_mask, hid=None):\n",
    "        '''\n",
    "            targ: (bs, targ_seq_len, targ_inp_size)\n",
    "            memory:(bs, src_seq_len, embed_dim)\n",
    "            memory_key_padding_mask: (bs, src_seq_len)\n",
    "            hid: (n_layers, bs, embed_dim)\n",
    "            returns:\n",
    "                output: (bs, tgt_seq_len, embed_dim)\n",
    "                attn_weight: (n_layers, bs, tgt_seq_len, src_seq_len)\n",
    "                hid: (n_layers, bs, targ_seq_len)\n",
    "        '''\n",
    "        assert hid.shape[0] == self.n_layers\n",
    "        new_hid = []\n",
    "        attn_weights = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            targ, attn_weight, h = layer(targ, memory, memory_key_padding_mask, None if hid is None else hid[i])\n",
    "            new_hid.append(h)\n",
    "            attn_weights.append(attn_weight)\n",
    "        return targ, torch.stack(attn_weights, 0), torch.stack(new_hid, 0)\n",
    "    def reset(self):\n",
    "        for layer in self.layers: layer.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ = torch.randn((16, 18, 20))\n",
    "memory = torch.randn((16, 28, 10))\n",
    "memory_key_padding_mask = torch.zeros((16, 28)).bool()\n",
    "hid = torch.randn((2, 16, 10))\n",
    "cross_attn_qrnn = CrossAttnQRNN(20, 10, 2)\n",
    "output, attn_weight, new_hid = cross_attn_qrnn(targ, memory, memory_key_padding_mask, hid)\n",
    "test_eq(output.shape, (16, 18, 10))\n",
    "test_eq(attn_weight.shape, (2, 16, 18, 28))\n",
    "test_eq(new_hid.shape, (2, 16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CrossAttnQRNNDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size, embeded_size, pad_id,\n",
    "        num_layers=1, drop_p=0,\n",
    "        num_heads=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embeded_size, padding_idx=pad_id)\n",
    "        self.decoder = CrossAttnQRNN(embeded_size, embeded_size, n_layers=num_layers, num_heads=num_heads, drop_p=drop_p)\n",
    "        self.classifier = nn.Linear(embeded_size, vocab_size)\n",
    "        \n",
    "    def forward(self, tgt_inp_ids, memory, memory_key_padding_mask, h):\n",
    "        '''\n",
    "            tgt_inp_ids: (bs, dec_seq_len)\n",
    "            memory:(bs, src_seq_len, embed_dim)\n",
    "            memory_key_padding_mask: (bs, src_seq_len)\n",
    "            h: (num_decoder_layers, bs, embeded_size)\n",
    "            returns: output, h\n",
    "                output: (bs, dec_seq_len, dec_vocab_size)\n",
    "                attn_weight: \n",
    "                h: (num_decoder_layers, bs, embeded_size)\n",
    "        '''\n",
    "        embeded = self.embedding(tgt_inp_ids) # (bs, dec_seq_len, embeded_size)\n",
    "        output, attn_weight, h = self.decoder(embeded, memory, memory_key_padding_mask, h) # (bs, dec_seq_len, embeded_size), (num_decoder_layers, bs, embeded_size)\n",
    "        output = self.classifier(output) # (bs, dec_seq_len, dec_vocab_size)\n",
    "        return output, attn_weight, h # (bs, dec_seq_len, dec_vocab_size), (num_decoder_layers, bs, embeded_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_inp_ids = torch.randint(0, 100, (16, 18))\n",
    "memory = torch.randn((16, 28, 10))\n",
    "memory_key_padding_mask = torch.zeros((16, 28)).bool()\n",
    "hid = torch.randn((2, 16, 10))\n",
    "cross_attn_qrnn_decoder = CrossAttnQRNNDecoder(100, 10, 1, num_layers=2)\n",
    "output, attn_weight, new_hid = cross_attn_qrnn_decoder(tgt_inp_ids, memory, memory_key_padding_mask, hid)\n",
    "test_eq(output.shape, (16, 18, 100))\n",
    "test_eq(attn_weight.shape, (2, 16, 18, 28))\n",
    "test_eq(new_hid.shape, (2, 16, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QRNN2QRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class QRNN2QRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        encoder: QRNNEncoder, decoder: QRNNDecoder, \n",
    "        num_encoder_layers, num_decoder_layers,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.proj = nn.Linear(2*num_encoder_layers, num_decoder_layers)\n",
    "        \n",
    "    def forward(self, src_input_ids, tgt_input_ids):\n",
    "        '''\n",
    "            src_input_ids: (bs, enc_seq_len)\n",
    "            tgt_input_ids: (bs, dec_seq_len)\n",
    "            returns: (bs, dec_seq_len, dec_vocab_size)\n",
    "        '''\n",
    "        _, enc_h = self.encoder(src_input_ids) # (2*num_encoder_layers, bs, embeded_size)\n",
    "        enc_h = enc_h.permute(1, 2, 0) # (bs, embeded_size, 2*num_encoder_layers)\n",
    "        enc_h_proj = self.proj(enc_h) # (bs, embeded_size, num_decoder_layers)\n",
    "        enc_h_proj = enc_h_proj.permute(2, 0, 1) # (num_decoder_layers, bs, embeded_size)\n",
    "        enc_h_proj = enc_h_proj.contiguous()\n",
    "        \n",
    "        output, dec_h = self.decoder(tgt_input_ids, enc_h_proj) # (bs, dec_seq_len, dec_vocab_size), (num_decoder_layers, bs, embeded_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrnn2qrnn = QRNN2QRNN(encoder, decoder, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "src_input_ids = torch.randint(0, enc_vocab_size, (bs, enc_seq_len)) # (bs, enc_seq_len)\n",
    "tgt_input_ids = torch.randint(0, dec_vocab_size, (bs, dec_seq_len)) # (bs, dec_seq_len)\n",
    "output = qrnn2qrnn(src_input_ids, tgt_input_ids) # (bs, dec_seq_len, dec_vocab_size)\n",
    "test_eq(output.shape, (bs, dec_seq_len, dec_vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QRNN2AttnQRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class QRNN2AttnQRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        encoder: QRNNEncoder, decoder: CrossAttnQRNNDecoder, \n",
    "        num_encoder_layers, num_decoder_layers, embeded_size, # for project\n",
    "        enc_pad_id,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.hid_proj = nn.Linear(2*num_encoder_layers, num_decoder_layers)\n",
    "        self.memory_proj = nn.Linear(2*embeded_size, embeded_size)\n",
    "        \n",
    "        self.enc_pad_id = enc_pad_id\n",
    "        \n",
    "    def forward(self, src_input_ids, tgt_input_ids):\n",
    "        '''\n",
    "            src_input_ids: (bs, enc_seq_len)\n",
    "            tgt_input_ids: (bs, dec_seq_len)\n",
    "            returns: (bs, dec_seq_len, dec_vocab_size)\n",
    "        '''\n",
    "        memory, enc_h = self.encoder(src_input_ids) # (bs, enc_seq_len, 2*embeded_size), (2*num_encoder_layers, bs, embeded_size)\n",
    "        memory_proj = self.memory_proj(memory) # (bs, enc_seq_len, embeded_size)\n",
    "        enc_h = enc_h.permute(1, 2, 0) # (bs, embeded_size, 2*num_encoder_layers)\n",
    "        enc_h_proj = self.hid_proj(enc_h) # (bs, embeded_size, num_decoder_layers)\n",
    "        enc_h_proj = enc_h_proj.permute(2, 0, 1) # (num_decoder_layers, bs, embeded_size)\n",
    "        enc_h_proj = enc_h_proj.contiguous()\n",
    "        memory_key_padding_mask = gen_key_padding_mask(src_input_ids, self.enc_pad_id)\n",
    "        \n",
    "        output, _, dec_h = self.decoder(tgt_input_ids, memory_proj, memory_key_padding_mask, enc_h_proj) # (bs, dec_seq_len, dec_vocab_size), (num_decoder_layers, bs, embeded_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_attn_qrnn_decoder = CrossAttnQRNNDecoder(dec_vocab_size, embeded_size, dec_pad_id, num_layers=2)\n",
    "qrnn2attnqrnn = QRNN2AttnQRNN(encoder, cross_attn_qrnn_decoder, num_encoder_layers, num_decoder_layers, embeded_size, enc_pad_id)\n",
    "\n",
    "src_input_ids = torch.randint(0, enc_vocab_size, (bs, enc_seq_len)) # (bs, enc_seq_len)\n",
    "tgt_input_ids = torch.randint(0, dec_vocab_size, (bs, dec_seq_len)) # (bs, dec_seq_len)\n",
    "output = qrnn2attnqrnn(src_input_ids, tgt_input_ids) # (bs, dec_seq_len, dec_vocab_size)\n",
    "test_eq(output.shape, (bs, dec_seq_len, dec_vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeneratedQRNN2QRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-bert-wwm-ext')\n",
    "dec_tokenizer = GPT2DecoderTokenizer.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class GeneratedQRNN2QRNN():\n",
    "    '''\n",
    "        device is for created tensors\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq2seq: QRNN2QRNN, \n",
    "        enc_tokenizer: PreTrainedTokenizer,\n",
    "        dec_tokenizer: PreTrainedTokenizer,\n",
    "    ):\n",
    "        self.seq2seq = seq2seq\n",
    "        self.enc_tokenizer = enc_tokenizer\n",
    "        self.dec_tokenizer = dec_tokenizer\n",
    "        self.generatedLM = GeneratedLM(seq2seq.decoder, len(dec_tokenizer), dec_tokenizer.pad_token_id, [dec_tokenizer.eos_token_id], support_past=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@patch\n",
    "@torch.no_grad()\n",
    "def generate_from_ids(self: GeneratedQRNN2QRNN, src_input_ids, generate_args: GenerateArgs):\n",
    "    ''' src_input_ids: (bs, enc_seq_len) '''\n",
    "    self.seq2seq.eval()\n",
    "    self.seq2seq.decoder.decoder.reset()\n",
    "    \n",
    "    device = src_input_ids.device\n",
    "    bs = src_input_ids.shape[0]\n",
    "    tgt_input_ids = torch.zeros((bs, 1), dtype=torch.long).fill_(self.dec_tokenizer.bos_token_id).to(device) # (bs, 1)\n",
    "\n",
    "    _, enc_h = self.seq2seq.encoder(src_input_ids) # (2*num_encoder_layers, bs, embeded_size)\n",
    "    enc_h = enc_h.permute(1, 2, 0) # (bs, embeded_size, 2*num_encoder_layers)\n",
    "    enc_h_proj =  self.seq2seq.proj(enc_h) # (bs, embeded_size, num_decoder_layers)\n",
    "    model_otherargs = self.generatedLM.build_model_otherargs_for_beam([enc_h_proj], generate_args.num_beams) # (bs*num_beams, embeded_size, num_decoder_layers)\n",
    "    enc_h_proj = model_otherargs[0].permute(2, 0, 1) # (num_decoder_layers, bs*num_beams, embeded_size)\n",
    "    enc_h_proj = enc_h_proj.contiguous()\n",
    "\n",
    "    result = self.generatedLM.generate(tgt_input_ids, generate_args, [enc_h_proj])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_qrnn2qrnn = GeneratedQRNN2QRNN(qrnn2qrnn, enc_tokenizer, dec_tokenizer)\n",
    "\n",
    "generate_args = GenerateArgs(max_length=20, num_beams=2)\n",
    "src_input_ids = torch.randint(0, enc_vocab_size, (bs, enc_seq_len)) # (bs, enc_seq_len)\n",
    "result = generated_qrnn2qrnn.generate_from_ids(src_input_ids, generate_args)\n",
    "test_eq(result.shape, (bs, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeneratedQRNN2AttnQRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class GeneratedQRNN2AttnQRNN():\n",
    "    '''\n",
    "        device is for created tensors\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq2seq: QRNN2AttnQRNN, \n",
    "        enc_tokenizer: PreTrainedTokenizer,\n",
    "        dec_tokenizer: PreTrainedTokenizer,\n",
    "    ):\n",
    "        self.seq2seq = seq2seq\n",
    "        self.enc_tokenizer = enc_tokenizer\n",
    "        self.dec_tokenizer = dec_tokenizer\n",
    "        self.generatedLM = GeneratedLM(seq2seq.decoder, len(dec_tokenizer), dec_tokenizer.pad_token_id, [dec_tokenizer.eos_token_id], support_past=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@patch\n",
    "@torch.no_grad()\n",
    "def generate_from_ids(self: GeneratedQRNN2AttnQRNN, src_input_ids, generate_args: GenerateArgs):\n",
    "    ''' src_input_ids: (bs, enc_seq_len) '''\n",
    "    self.seq2seq.eval()\n",
    "    self.seq2seq.decoder.decoder.reset()\n",
    "    \n",
    "    device = src_input_ids.device\n",
    "    bs = src_input_ids.shape[0]\n",
    "    tgt_input_ids = torch.zeros((bs, 1), dtype=torch.long).fill_(self.dec_tokenizer.bos_token_id).to(device) # (bs, 1)    \n",
    "    \n",
    "    memory, enc_h = self.seq2seq.encoder(src_input_ids) # (bs, enc_seq_len, 2*embeded_size), (2*num_encoder_layers, bs, embeded_size)\n",
    "    memory_proj = self.seq2seq.memory_proj(memory) # (bs, enc_seq_len, embeded_size)\n",
    "    memory_key_padding_mask = gen_key_padding_mask(src_input_ids, self.seq2seq.enc_pad_id)\n",
    "    enc_h = enc_h.permute(1, 2, 0) # (bs, embeded_size, 2*num_encoder_layers)\n",
    "    enc_h_proj = self.seq2seq.hid_proj(enc_h) # (bs, embeded_size, num_decoder_layers)\n",
    "    memory_proj, memory_key_padding_mask, enc_h_proj = self.generatedLM.build_model_otherargs_for_beam([memory_proj, memory_key_padding_mask, enc_h_proj], generate_args.num_beams) # (bs*num_beams, embeded_size, num_decoder_layers)\n",
    "    enc_h_proj = enc_h_proj.permute(2, 0, 1) # (num_decoder_layers, bs, embeded_size)\n",
    "    enc_h_proj = enc_h_proj.contiguous()\n",
    "\n",
    "    result = self.generatedLM.generate(tgt_input_ids, generate_args, [memory_proj, memory_key_padding_mask, enc_h_proj])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_qrnn2attnqrnn = GeneratedQRNN2AttnQRNN(qrnn2attnqrnn, enc_tokenizer, dec_tokenizer)\n",
    "\n",
    "generate_args = GenerateArgs(max_length=20, num_beams=2)\n",
    "src_input_ids = torch.randint(0, enc_vocab_size, (bs, enc_seq_len)) # (bs, enc_seq_len)\n",
    "result = generated_qrnn2attnqrnn.generate_from_ids(src_input_ids, generate_args)\n",
    "test_eq(result.shape, (bs, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class QRNN2QRNNCallback(Callback):\n",
    "    def after_batch(self):\n",
    "        self.learn.model.decoder.decoder.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 02_data.news_commentary.ipynb.\n",
      "Converted 02_data.tatoeba.ipynb.\n",
      "Converted 03a_models.patch.ipynb.\n",
      "Converted 03c_models.bert2gpt2.ipynb.\n",
      "Converted 03c_models.gru2gru.ipynb.\n",
      "Converted 03c_models.qrnn2qrnn.ipynb.\n",
      "Converted 03c_models.tran2tran.ipynb.\n",
      "Converted 04_metrics.ipynb.\n",
      "Converted 99_fulltest_bert2gpt2.ipynb.\n",
      "Converted 99_fulltest_gru2gru.ipynb.\n",
      "Converted 99_fulltest_qrnn2attnqrnn.ipynb.\n",
      "Converted 99_fulltest_qrnn2qrnn.ipynb.\n",
      "Converted 99_fulltest_tran2tran.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
