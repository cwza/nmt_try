{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "from fastai2.basics import *\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, PreTrainedTokenizer\n",
    "\n",
    "from fastai2_utils.pytorch.transformer import *\n",
    "from fastai_transformers_utils.tokenizers import GPT2DecoderTokenizer\n",
    "from fastai_transformers_utils.generated_lm import GeneratedLM, GenerateArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.bert2gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model_name = 'hfl/chinese-bert-wwm-ext'\n",
    "dec_model_name = 'distilgpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_tokenizer = AutoTokenizer.from_pretrained(enc_model_name)\n",
    "dec_tokenizer = GPT2DecoderTokenizer.from_pretrained(dec_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Bert2GPT2\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def gen_attention_mask(inp_ids, pad_id):\n",
    "    '''\n",
    "        Returns Tensor where 0 are positions that contain pad_id, others 1.\n",
    "        input_ids: (bs, seq_len) returns: (bs, seq_len)\n",
    "    '''\n",
    "    key_padding_mask = gen_key_padding_mask(inp_ids, pad_id)\n",
    "    return (~key_padding_mask).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[12, 11, 0, 0], \n",
    "                          [9, 1, 5, 0]])\n",
    "attention_mask = gen_attention_mask(input_ids, 0)\n",
    "test_eq(attention_mask, torch.tensor([[1, 1, 0, 0],\n",
    "                                      [1, 1, 1, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        ''' model_name: pretrained bert model name from huggingface '''\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.layer_groups = [self.bert.embeddings, *self.bert.encoder.layer, self.bert.pooler]\n",
    "    def forward(self, src_input_ids, src_attention_mask):\n",
    "        '''\n",
    "        src_input_ids: (bs, enc_seq_len)\n",
    "        src_attention_mask: (bs, enc_seq_len)\n",
    "        returns: (bs, enc_seq_len, embed_size)\n",
    "        '''\n",
    "        return self.bert(src_input_ids, attention_mask=src_attention_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101, 3947, 6275,  102,    0,    0,    0,    0,    0,    0],\n",
       "         [ 101,  872, 1962, 1621,  102,    0,    0,    0,    0,    0],\n",
       "         [ 101, 3193, 2128,  102,    0,    0,    0,    0,    0,    0]]),\n",
       " tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_seq_len = 10\n",
    "src_strs = ['測試', '你好嗎', '早安']\n",
    "src_input_ids = torch.tensor([enc_tokenizer.encode(src_str, max_length=enc_seq_len, pad_to_max_length=True) for src_str in src_strs])\n",
    "src_attention_mask = gen_attention_mask(src_input_ids, enc_tokenizer.pad_token_id)\n",
    "src_input_ids, src_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = BertEncoder(enc_model_name)\n",
    "test_eq(encoder(src_input_ids, src_attention_mask).shape, (3, enc_seq_len, encoder.bert.config.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _adujsted_gpt2wte(gpt2):\n",
    "    ''' Adjust pretrained gpt2 wte layer to adapt the GPT2DecoderTokenizer.\n",
    "    Add bos_token and pad_token at the last of gpt2.wte.\n",
    "    Use GPT2DecoderTokenizer or make sure the pad token is at the last of your tokenizer and the bos token is at the second-last.\n",
    "    '''\n",
    "    old_wte = gpt2.wte\n",
    "    old_weight = old_wte.weight\n",
    "    num_embeddings = old_wte.num_embeddings+2\n",
    "    embedding_dim = old_wte.embedding_dim\n",
    "    \n",
    "    bos_weight = old_weight.mean(dim=0)[None] # (1, embedding_dim)\n",
    "    pad_weight = torch.zeros((1, embedding_dim))\n",
    "    \n",
    "    new_weight = torch.cat([old_weight, bos_weight, pad_weight], dim=0) # (num_embeddings, embedding_dim)\n",
    "    new_wte = nn.Embedding(num_embeddings, embedding_dim, padding_idx=num_embeddings-1)\n",
    "    new_wte.weight.data = new_weight\n",
    "    \n",
    "    return new_wte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = AutoModel.from_pretrained(dec_model_name)\n",
    "old_wte = gpt2.wte\n",
    "new_wte = _adujsted_gpt2wte(gpt2)\n",
    "\n",
    "test_eq(old_wte.weight, new_wte.weight[:-2])\n",
    "test_eq(new_wte.weight[-1], torch.zeros((old_wte.embedding_dim))) # zero\n",
    "test_eq(new_wte.weight[-2], old_wte.weight.mean(dim=0)) # mean of old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "class GPT2Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name, pad_id, # for GPT2\n",
    "        vocab_size, # for classifier\n",
    "        num_heads=1, drop_p=0, num_layers=1, # for CrossAttention\n",
    "    ):\n",
    "        ''' model_name: pretrained gpt2 model name from huggingface '''\n",
    "        super().__init__()\n",
    "        self.gpt2 = AutoModel.from_pretrained(model_name)\n",
    "        self.gpt2.wte = _adujsted_gpt2wte(self.gpt2)\n",
    "        self.cross_attn = CrossAttention(self.gpt2.config.n_embd, num_heads, drop_p, num_layers)\n",
    "        self.classifier = nn.Linear(self.gpt2.config.n_embd, vocab_size)\n",
    "        \n",
    "        self.pad_id = pad_id\n",
    "        self.layer_groups = [\n",
    "            self.gpt2.wte, self.gpt2.wpe, *self.gpt2.h, self.gpt2.ln_f, *self.cross_attn.cross_attn_layers, self.classifier\n",
    "        ]\n",
    "    def forward(self, tgt_input_ids, memory, memory_key_padding_mask):\n",
    "        '''\n",
    "            tgt_input_ids: (bs, dec_seq_len)\n",
    "            memory: (bs, enc_seq_len, embed_size)\n",
    "            memory_key_padding_mask: (bs, enc_seq_len)\n",
    "            returns: output, attn_weight\n",
    "                output: (bs, dec_seq_len, dec_vocab_size)\n",
    "                attn_weight: (bs, dec_seq_len, enc_seq_len)\n",
    "        '''\n",
    "        tgt_attention_mask = gen_attention_mask(tgt_input_ids, self.pad_id) # (bs, dec_seq_len)\n",
    "        gpt2_out = self.gpt2(tgt_input_ids, attention_mask=tgt_attention_mask)[0] # (bs, dec_seq_len, 768)\n",
    "        attn_output, attn_weight = self.cross_attn(gpt2_out, memory, src_key_padding_mask=memory_key_padding_mask) # (bs, dec_seq_len, 768), (bs, dec_seq_len, enc_seq_len)\n",
    "        \n",
    "        output = self.classifier(attn_output) # (bs, dec_seq_len, dec_vocab_size)\n",
    "        \n",
    "        return output, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = GPT2Decoder(\n",
    "    dec_model_name, dec_tokenizer.pad_token_id,\n",
    "    vocab_size=len(dec_tokenizer),\n",
    "    num_heads=2, drop_p=0, num_layers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test embedding\n",
    "test_eq((decoder.gpt2.wte.num_embeddings, decoder.gpt2.wte.embedding_dim), (gpt2.wte.num_embeddings+2, gpt2.wte.embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward shape\n",
    "tgt_input_ids = torch.randint(0, 50259, (3, 40))\n",
    "memory = torch.randn((3, 50, 768))\n",
    "memory_key_padding_mask = torch.zeros((3, 50)).bool()\n",
    "\n",
    "output, attn_weight = decoder(tgt_input_ids, memory, memory_key_padding_mask)\n",
    "test_eq(output.shape, (3, 40, len(dec_tokenizer)))\n",
    "test_eq(attn_weight.shape, (3, 40, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert2Gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Bert2GPT2(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        encoder: BertEncoder, decoder: GPT2Decoder,\n",
    "        enc_pad_id, # for src_key_padding_mask and memory_key_padding_mask\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.enc_pad_id = enc_pad_id\n",
    "    def forward(self, src_input_ids, tgt_input_ids):\n",
    "        '''\n",
    "            src_input_ids: (bs, enc_seq_len)\n",
    "            tgt_input_ids: (bs, dec_seq_len)\n",
    "        '''\n",
    "        src_attention_mask = gen_attention_mask(src_input_ids, self.enc_pad_id) # (bs, enc_seq_len)\n",
    "        memory = self.encoder(src_input_ids, src_attention_mask) # (bs, enc_seq_len, embed_size)\n",
    "        memory_key_padding_mask = (1-src_attention_mask).bool()\n",
    "        output, _ = self.decoder(tgt_input_ids, memory, memory_key_padding_mask=memory_key_padding_mask) # (bs, dec_seq_len, embeded_size)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert2gpt2 = Bert2GPT2(encoder, decoder, enc_tokenizer.pad_token_id)\n",
    "src_input_ids = torch.randint(0, len(enc_tokenizer), (3, 50)) # (bs, enc_seq_len)\n",
    "tgt_input_ids = torch.randint(0, len(dec_tokenizer), (3, 40)) # (bs, dec_seq_len)\n",
    "\n",
    "output = bert2gpt2(src_input_ids, tgt_input_ids)\n",
    "test_eq(output.shape, (3, 40, len(dec_tokenizer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeneratedBert2GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class GeneratedBert2GPT2():\n",
    "    def __init__(\n",
    "        self, \n",
    "        seq2seq: Bert2GPT2, \n",
    "        enc_tokenizer: PreTrainedTokenizer,\n",
    "        dec_tokenizer: PreTrainedTokenizer,\n",
    "    ):\n",
    "        self.seq2seq = seq2seq\n",
    "        self.enc_tokenizer = enc_tokenizer\n",
    "        self.dec_tokenizer = dec_tokenizer\n",
    "        self.generatedLM = GeneratedLM(seq2seq.decoder, len(dec_tokenizer), dec_tokenizer.pad_token_id, [dec_tokenizer.eos_token_id], support_past=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_from_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@patch\n",
    "@torch.no_grad()\n",
    "def generate_from_ids(self: GeneratedBert2GPT2, src_input_ids, generate_args: GenerateArgs):\n",
    "    ''' src_input_ids: (bs, enc_seq_len), returns: (bs, max_length)'''\n",
    "    self.seq2seq.eval()\n",
    "    \n",
    "    device = src_input_ids.device\n",
    "    bs = src_input_ids.shape[0]\n",
    "    tgt_input_ids = torch.zeros((bs, 1), dtype=torch.long, device=device).fill_(self.dec_tokenizer.bos_token_id) # (bs, 1)\n",
    "\n",
    "    src_attention_mask = gen_attention_mask(src_input_ids, self.enc_tokenizer.pad_token_id) # (bs, enc_seq_len)\n",
    "    memory = self.seq2seq.encoder(src_input_ids, src_attention_mask) # (bs, enc_seq_len, embed_size)\n",
    "    memory_key_padding_mask = (1-src_attention_mask).bool()\n",
    "    model_otherargs = self.generatedLM.build_model_otherargs_for_beam([memory, memory_key_padding_mask], generate_args.num_beams)\n",
    "\n",
    "    result = self.generatedLM.generate(tgt_input_ids, generate_args, [model_otherargs[0]], dict(memory_key_padding_mask=model_otherargs[1]))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_bert2gpt2 = GeneratedBert2GPT2(bert2gpt2, enc_tokenizer, dec_tokenizer)\n",
    "\n",
    "generate_args = GenerateArgs(max_length=10, num_beams=2)\n",
    "src_input_ids = torch.randint(0, len(enc_tokenizer), (3, 50)) # (bs, enc_seq_len)\n",
    "result = generated_bert2gpt2.generate_from_ids(src_input_ids, generate_args)\n",
    "test_eq(result.shape, (3, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 02_data.news_commentary.ipynb.\n",
      "Converted 02_data.tatoeba.ipynb.\n",
      "Converted 03a_models.patch.ipynb.\n",
      "Converted 03c_models.bert2gpt2.ipynb.\n",
      "Converted 03c_models.gru2gru.ipynb.\n",
      "Converted 03c_models.qrnn2qrnn.ipynb.\n",
      "Converted 03c_models.tran2tran.ipynb.\n",
      "Converted 04_metrics.ipynb.\n",
      "Converted 99_fulltest_bert2gpt2.ipynb.\n",
      "Converted 99_fulltest_gru2gru.ipynb.\n",
      "Converted 99_fulltest_qrnn2qrnn.ipynb.\n",
      "Converted 99_fulltest_tran2tran.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
